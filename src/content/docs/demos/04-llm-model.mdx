---
title: LLM Model Demo
description: Deploy and test LLM models integrated with OpenAI API.
---

import { Steps, Aside, Tabs, TabItem } from '@astrojs/starlight/components';

This guide demonstrates how to deploy a Large Language Model (LLM) in your OtterScale cluster and test it using Python with OpenAI API integration.

## Prerequisites

Ensure you have the following:

- **Python 3.8+**: For running the test scripts
- **OpenAI API Key**: Obtain from [OpenAI Platform](https://platform.openai.com/api-keys)
- **Python Libraries**: `requests` and `openai`

```bash
pip install requests openai
```

## Deploy LLM Model

<Aside type="tip">
This guide demonstrates deploying an LLM model (e.g., meta-llama/Llama-2-7b-chat) to your OtterScale cluster using the Models page. You can deploy other models from the model artifact repositories as well.
</Aside>

<Steps>

1. Navigate to the **Models** page in your OtterScale cluster.

2. Click the **Create** button to create a new model.

3. Select a model from your model artifacts:
   - Search for available models using the cloud icon in the search box
   - Or click the archive icon to browse model artifacts
   - Select your desired LLM (e.g., `meta-llama/Llama-2-7b-chat`)

4. Configure the model deployment:
   - **Name**: Choose a descriptive name (e.g., `llm-demo`)
   - **Namespace**: Select your target namespace
   - **Prefill Configuration**: Set vGPU memory %, replica count, and tensor configuration (if needed)
   - **Decode Configuration**: Set decoding parameters similarly
   - **Description**: Add any relevant notes about the deployment

5. Review the configuration and click **Create** to deploy the model.

6. Monitor the deployment status on the Models page. The status will change from `Pending` → `Running` → `Ready`.

7. Once the status shows **Ready**, click the **Test** button to verify the model API is working.

</Steps>

<Aside type="note">
The model deployment may take several minutes depending on the model size and available resources. Check the Pods table in the model details for specific pod status information.
</Aside>

## Test with Python

Once your LLM model is deployed and ready, you can test it using Python with OpenAI API integration.

<Aside title="Prerequisites">
Ensure you have Python and required libraries installed:

```bash
pip install requests openai
```

Obtain your OpenAI API key from [OpenAI Platform](https://platform.openai.com/api-keys).
</Aside>

### Connection Information

Before running the test scripts, you'll need:
- **OpenAI API Key**: Your API key from OpenAI
- **Model Name**: The model you created (e.g., `llm-demo`)
- **API Base URL**: Optional, if using a custom endpoint

<Tabs>


<TabItem label="Simple Question">

```python
import requests
import json

# Configuration
SERVICE_URL = "<your_service_url>"  # e.g., http://localhost:8000
MODEL_NAME = "<your_model_name>"    # e.g., llm-demo
MODEL_ID = "RedHatAI/Llama-3.2-1B-Instruct-FP8"

def ask_question(question):
    """Send a simple question to the LLM and get a response."""
    headers = {
        "OtterScale-Model-Name": MODEL_NAME,
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": MODEL_ID,
        "prompt": question
    }
    
    try:
        response = requests.post(
            f"{SERVICE_URL}/v1/chat",
            headers=headers,
            json=payload
        )
        response.raise_for_status()
        result = response.json()
        return result.get("response", result)
    except Exception as e:
        return f"✗ Error: {str(e)}"

# Test
question = "Are you alive? Please respond if you can process this message."
answer = ask_question(question)
print(f"Q: {question}")
print(f"A: {answer}")
```

</TabItem>

<TabItem label="Conversation">

```python
import requests
import json

# Configuration
SERVICE_URL = "<your_service_url>"  # e.g., http://localhost:8000
MODEL_NAME = "<your_model_name>"    # e.g., llm-demo
MODEL_ID = "RedHatAI/Llama-3.2-1B-Instruct-FP8"

class LLMChat:
    def __init__(self, service_url, model_name, model_id):
        self.service_url = service_url
        self.model_name = model_name
        self.model_id = model_id
        self.conversation = []
    
    def add_message(self, role, content):
        """Add a message to the conversation history."""
        self.conversation.append({"role": role, "content": content})
    
    def send_message(self, user_message):
        """Send a user message and get a response."""
        self.add_message("user", user_message)
        
        headers = {
            "OtterScale-Model-Name": self.model_name,
            "Content-Type": "application/json"
        }
        
        # Build context from conversation history
        context = "\n".join([f"{msg['role']}: {msg['content']}" for msg in self.conversation])
        
        payload = {
            "model": self.model_id,
            "prompt": context
        }
        
        try:
            response = requests.post(
                f"{self.service_url}/v1/chat",
                headers=headers,
                json=payload
            )
            response.raise_for_status()
            result = response.json()
            assistant_response = result.get("response", str(result))
            self.add_message("assistant", assistant_response)
            return assistant_response
        except Exception as e:
            return f"✗ Error: {str(e)}"
    
    def clear_history(self):
        """Clear conversation history."""
        self.conversation = []

# Test
chat = LLMChat(service_url=SERVICE_URL, model_name=MODEL_NAME, model_id=MODEL_ID)

# First message
response1 = chat.send_message("What are the three main colors of the French flag?")
print(f"Q: What are the three main colors of the French flag?")
print(f"A: {response1}\n")

# Follow-up message (maintains context)
response2 = chat.send_message("Which one represents liberty?")
print(f"Q: Which one represents liberty?")
print(f"A: {response2}\n")
```

</TabItem>

<TabItem label="Complete Example">

```python
import requests
import json

# Configuration
SERVICE_URL = "<your_service_url>"  # e.g., http://localhost:8000
MODEL_NAME = "<your_model_name>"    # e.g., llm-demo
MODEL_ID = "RedHatAI/Llama-3.2-1B-Instruct-FP8"

class LLMDemo:
    def __init__(self, service_url, model_name, model_id):
        self.service_url = service_url
        self.model_name = model_name
        self.model_id = model_id
        self.conversation = []
    
    def test_connection(self):
        """Test if the service is accessible."""
        try:
            headers = {
                "OtterScale-Model-Name": self.model_name,
                "Content-Type": "application/json"
            }
            payload = {
                "model": self.model_id,
                "prompt": "Hello"
            }
            response = requests.post(
                f"{self.service_url}/v1/chat",
                headers=headers,
                json=payload,
                timeout=10
            )
            response.raise_for_status()
            print("✓ Connection successful!")
            return True
        except Exception as e:
            print(f"✗ Connection failed: {str(e)}")
            return False
    
    def send_message(self, user_message):
        """Send a message and get a response."""
        self.conversation.append({"role": "user", "content": user_message})
        
        headers = {
            "OtterScale-Model-Name": self.model_name,
            "Content-Type": "application/json"
        }
        
        # Build context from conversation history
        context = "\n".join([f"{msg['role']}: {msg['content']}" for msg in self.conversation])
        
        payload = {
            "model": self.model_id,
            "prompt": context
        }
        
        try:
            response = requests.post(
                f"{self.service_url}/v1/chat",
                headers=headers,
                json=payload
            )
            response.raise_for_status()
            result = response.json()
            assistant_response = result.get("response", str(result))
            self.conversation.append({"role": "assistant", "content": assistant_response})
            return assistant_response
        except Exception as e:
            return f"✗ Error: {str(e)}"
    
    def clear_history(self):
        """Clear conversation history."""
        self.conversation = []

# Usage
if __name__ == "__main__":
    demo = LLMDemo(service_url=SERVICE_URL, model_name=MODEL_NAME, model_id=MODEL_ID)
    
    # Test connection
    demo.test_connection()
    
    # Start conversation
    response1 = demo.send_message("Tell me about artificial intelligence in 2 sentences.")
    print(f"Q: Tell me about artificial intelligence in 2 sentences.")
    print(f"A: {response1}\n")
    
    # Follow-up questions
    response2 = demo.send_message("What are the main applications?")
    print(f"Q: What are the main applications?")
    print(f"A: {response2}\n")
    
    response3 = demo.send_message("How does machine learning fit into this?")
    print(f"Q: How does machine learning fit into this?")
    print(f"A: {response3}")
```

</TabItem>
</Tabs>

<Aside type="note">
Replace `<your_openai_api_key>` with your actual OpenAI API key and `<your_model_name>` with your deployed model name. The examples use the OpenAI API format, which is compatible with many LLM services.
</Aside>