---
title: Models
description: Manage and monitor Large Language Models.
---

import { Steps, Aside } from '@astrojs/starlight/components';

The Models section allows you to manage and monitor Large Language Models (LLMs) deployed in the OtterScale cluster. It provides real-time insights into model performance and resource usage.

## Overview

The Models page displays a list of deployed LLMs. The table includes the following information:

| Column                  | Description                                                                               |
| :---------------------- | :---------------------------------------------------------------------------------------- |
| **Model**               | The name of the application hosting the model. Links to the application details.          |
| **Name**                | The specific name or identifier of the LLM (e.g., `llama3-8b`).                           |
| **Replicas**            | The number of desired replicas for the model service.                                     |
| **Healthies**           | The number of currently healthy and ready replicas.                                       |
| **GPU Cache**           | The percentage of GPU cache currently in use.                                             |
| **KV Cache**            | The percentage of Key-Value (KV) cache usage, critical for transformer model performance. |
| **Requests**            | The total request latency or load metric for the model.                                   |
| **Time to First Token** | The average time taken to generate the first token of a response (latency metric).        |

## Monitor Models

The dashboard integrates with Prometheus to provide real-time metrics for each model:

- **GPU & KV Cache**: Monitor these percentages to ensure your models are not running out of memory context, which could degrade performance or cause errors.
- **Latency Metrics**: Track "Requests" and "Time to First Token" to ensure the models are responsive and meeting service level objectives (SLOs).

## Manage Models

You can perform basic lifecycle actions on the models:

### Create a Model

To deploy a new model:

1.  Click the **Create** button (plus icon) at the top of the page.
2.  Follow the prompts to configure and deploy your LLM service.

### Delete a Model

To remove a model:

1.  Locate the model in the list.
2.  Click the **Delete** button (trash icon) in the actions menu.
3.  Confirm the action to remove the model deployment.
